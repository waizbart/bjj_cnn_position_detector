{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_SIZE = 256\n",
    "\n",
    "labels = {'standing': 0, 'takedown': 1, 'open_guard': 2, 'half_guard': 3, 'closed_guard': 4, '5050_guard': 5, 'side_control': 6, 'mount': 7, 'back': 8, 'turtle': 9}\n",
    "body_parts = [\"nose\", \"left eye\", \"right eye\", \"left ear\", \"right ear\", \"left shoulder\", \"right shoulder\", \"left elbow\", \"right elbow\", \"left wrist\", \"right wrist\", \"left hip\", \"right hip\", \"left knee\", \"right knee\", \"left ankle\", \"right ankle\"]\n",
    "num_labels = len(labels)\n",
    "num_keypoints = 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importando modelo movenet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mediapipe import solutions\n",
    "from mediapipe.framework.formats import landmark_pb2\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def draw_landmarks_on_image(rgb_image, detection_result):\n",
    "  pose_landmarks_list = detection_result.pose_landmarks\n",
    "  annotated_image = np.copy(rgb_image)\n",
    "\n",
    "  # Loop through the detected poses to visualize.\n",
    "  for idx in range(len(pose_landmarks_list)):\n",
    "    pose_landmarks = pose_landmarks_list[idx]\n",
    "    \n",
    "    np_landmarks = np.array([[landmark.x, landmark.y, landmark.z] for landmark in pose_landmarks])\n",
    "    \n",
    "    print(np_landmarks.shape)\n",
    "\n",
    "    # Draw the pose landmarks.\n",
    "    pose_landmarks_proto = landmark_pb2.NormalizedLandmarkList()\n",
    "    \n",
    "    pose_landmarks_proto.landmark.extend([\n",
    "      landmark_pb2.NormalizedLandmark(x=landmark.x, y=landmark.y, z=landmark.z) for landmark in pose_landmarks\n",
    "    ])\n",
    "    \n",
    "    \n",
    "    solutions.drawing_utils.draw_landmarks(\n",
    "      annotated_image,\n",
    "      pose_landmarks_proto,\n",
    "      solutions.pose.POSE_CONNECTIONS,\n",
    "      solutions.drawing_styles.get_default_pose_landmarks_style())\n",
    "  return annotated_image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(33, 3)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "-1"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STEP 1: Import the necessary modules.\n",
    "import mediapipe as mp\n",
    "from mediapipe.tasks import python\n",
    "from mediapipe.tasks.python import vision\n",
    "import cv2\n",
    "\n",
    "# STEP 2: Create an PoseLandmarker object.\n",
    "base_options = python.BaseOptions(model_asset_path='pose_landmarker_heavy.task')\n",
    "options = vision.PoseLandmarkerOptions(\n",
    "    base_options=base_options,\n",
    "    output_segmentation_masks=True)\n",
    "detector = vision.PoseLandmarker.create_from_options(options)\n",
    "\n",
    "# STEP 3: Load the input image.\n",
    "image = mp.Image.create_from_file(\"../../test_images/standing.jpg\")\n",
    "\n",
    "# STEP 4: Detect pose landmarks from the input image.\n",
    "detection_result = detector.detect(image)\n",
    "\n",
    "# STEP 5: Process the detection result. In this case, visualize it.\n",
    "annotated_image = draw_landmarks_on_image(image.numpy_view(), detection_result)\n",
    "rgb_annotated_image = cv2.cvtColor(annotated_image, cv2.COLOR_RGB2BGR)\n",
    "\n",
    "# STEP 6: Display the annotated image.\n",
    "cv2.imshow('MediaPipe Pose', rgb_annotated_image)\n",
    "cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Carregando modelo pose BJJ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\guilh\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\keras\\src\\backend.py:1398: The name tf.executing_eagerly_outside_functions is deprecated. Please use tf.compat.v1.executing_eagerly_outside_functions instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "model = tf.keras.models.load_model(\"jiu_model.h5\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Funções úteis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt # type: ignore\n",
    "import numpy as np\n",
    "\n",
    "def plot_movenet_prediction(img, keypoints_dict, label):\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.title('Original Image')\n",
    "    plt.imshow(img[0])\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.imshow(img[0])\n",
    "    plt.title('Pose')\n",
    "    plt.axis('off')\n",
    "    \n",
    "    for i in range(17):\n",
    "        plt.scatter(keypoints_dict[label[i]][1],keypoints_dict[label[i]][0],color='green')\n",
    "\n",
    "    connections = [\n",
    "        ('nose', 'left eye'), ('left eye', 'left ear'), ('nose', 'right eye'), ('right eye', 'right ear'),\n",
    "        ('nose', 'left shoulder'), ('left shoulder', 'left elbow'), ('left elbow', 'left wrist'),\n",
    "        ('nose', 'right shoulder'), ('right shoulder', 'right elbow'), ('right elbow', 'right wrist'),\n",
    "        ('left shoulder', 'left hip'), ('right shoulder', 'right hip'), ('left hip', 'right hip'),\n",
    "        ('left hip', 'left knee'), ('right hip', 'right knee'), ('left knee', 'left ankle'), ('right knee', 'right ankle')\n",
    "    ]\n",
    "\n",
    "    for start_key, end_key in connections:\n",
    "        if start_key in keypoints_dict and end_key in keypoints_dict:\n",
    "            start_point = keypoints_dict[start_key][:2]  # Take first two values\n",
    "            end_point = keypoints_dict[end_key][:2]      # Take first two values\n",
    "            plt.plot([start_point[1], end_point[1]], [start_point[0], end_point[0]], linewidth=2)\n",
    "    \n",
    "    plt.subplot(1, 3, 3)\n",
    "    plt.imshow((img[0]/255)/255)\n",
    "    plt.title('Only Pose Image')\n",
    "    for start_key, end_key in connections:\n",
    "        if start_key in keypoints_dict and end_key in keypoints_dict:\n",
    "            start_point = keypoints_dict[start_key][:2]  # Take first two values\n",
    "            end_point = keypoints_dict[end_key][:2]      # Take first two values\n",
    "            plt.plot([start_point[1], end_point[1]], [start_point[0], end_point[0]], linewidth=2)\n",
    "            \n",
    "def get_keypoints(image):\n",
    "    X = tf.expand_dims(image, axis=0)\n",
    "    X = tf.cast(tf.image.resize_with_pad(X, IMAGE_SIZE, IMAGE_SIZE), dtype=tf.int32)\n",
    "    img = tf.image.resize_with_pad(image, IMAGE_SIZE, IMAGE_SIZE)\n",
    "    img = tf.cast(img, dtype=tf.int32)\n",
    "    img = tf.expand_dims(img, axis=0)\n",
    "    img = img.numpy()\n",
    "    outputs = movenet(X)\n",
    "    keypoints = outputs['output_0'].numpy()\n",
    "    return keypoints, img\n",
    "\n",
    "def get_keypoints_dict(keypoints):\n",
    "    max_key,key_val = keypoints[0,:,55].argmax(),keypoints[0,:,55].max()\n",
    "    max_key,key_val\n",
    "    max_points = keypoints[0,max_key,:]\n",
    "    max_points = max_points*IMAGE_SIZE\n",
    "    max_points = max_points.astype(float)\n",
    "    keypoints_dict = {}\n",
    "    for i in range(0,len(max_points)-5,3):\n",
    "        keypoints_dict[body_parts[i//3]] = [max_points[i],max_points[i+1],max_points[i+2]]\n",
    "    return keypoints_dict\n",
    "\n",
    "def pose_prediction(keypoints):\n",
    "    # normalize keypoints\n",
    "    max_x = np.max(keypoints[:,0])\n",
    "    \n",
    "    keypoints = keypoints/max_x\n",
    "    \n",
    "    keypoints = keypoints[0]\n",
    "    \n",
    "    keypoints_batch = []\n",
    "    \n",
    "    for k in keypoints:\n",
    "        keypoints_batch.append(k[:51])\n",
    "        \n",
    "    keypoints_batch = np.array(keypoints_batch)\n",
    "    \n",
    "    predictions = model.predict(keypoints_batch)\n",
    "    \n",
    "    return np.argmax(predictions[0])    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação em imagem única"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'movenet' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mio\u001b[38;5;241m.\u001b[39mread_file(image_path)\n\u001b[0;32m      3\u001b[0m image \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mcompat\u001b[38;5;241m.\u001b[39mv1\u001b[38;5;241m.\u001b[39mimage\u001b[38;5;241m.\u001b[39mdecode_jpeg(image)\n\u001b[1;32m----> 5\u001b[0m keypoints, img \u001b[38;5;241m=\u001b[39m \u001b[43mget_keypoints\u001b[49m\u001b[43m(\u001b[49m\u001b[43mimage\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m keypoints_dict \u001b[38;5;241m=\u001b[39m get_keypoints_dict(keypoints)\n\u001b[0;32m      7\u001b[0m label_index \u001b[38;5;241m=\u001b[39m pose_prediction(keypoints)\n",
      "Cell \u001b[1;32mIn[5], line 47\u001b[0m, in \u001b[0;36mget_keypoints\u001b[1;34m(image)\u001b[0m\n\u001b[0;32m     45\u001b[0m img \u001b[38;5;241m=\u001b[39m tf\u001b[38;5;241m.\u001b[39mexpand_dims(img, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m     46\u001b[0m img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[1;32m---> 47\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmovenet\u001b[49m(X)\n\u001b[0;32m     48\u001b[0m keypoints \u001b[38;5;241m=\u001b[39m outputs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124moutput_0\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mnumpy()\n\u001b[0;32m     49\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m keypoints, img\n",
      "\u001b[1;31mNameError\u001b[0m: name 'movenet' is not defined"
     ]
    }
   ],
   "source": [
    "image_path = '../../test_images/standing02.jpg'\n",
    "image = tf.io.read_file(image_path)\n",
    "image = tf.compat.v1.image.decode_jpeg(image)\n",
    "\n",
    "keypoints, img = get_keypoints(image)\n",
    "keypoints_dict = get_keypoints_dict(keypoints)\n",
    "label_index = pose_prediction(keypoints)\n",
    "\n",
    "plot_movenet_prediction(img, keypoints_dict, body_parts)\n",
    "\n",
    "print(\"Predicted Label: \", list(labels.keys())[label_index])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Classificação em vídeo em tempo real"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2 # type: ignore\n",
    "\n",
    "cap = cv2.VideoCapture(\"../../test_videos/video.mp4\")\n",
    "\n",
    "while cap.isOpened():\n",
    "    \n",
    "    ret, frame = cap.read()\n",
    "    \n",
    "    if not ret:\n",
    "        break\n",
    "    \n",
    "    frame = cv2.resize(frame, (IMAGE_SIZE, IMAGE_SIZE))\n",
    "    \n",
    "    keypoints, img = get_keypoints(frame)\n",
    "    label_index = pose_prediction(keypoints)\n",
    "    \n",
    "    print(\"Predicted Label: \", list(labels.keys())[label_index])\n",
    "    \n",
    "    cv2.imshow(\"Frame\", frame)\n",
    "    \n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cap.release()\n",
    "        cv2.destroyAllWindows()\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
